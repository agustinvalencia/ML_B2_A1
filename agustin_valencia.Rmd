---
title: "Machine Learning - Block02 Assignment 01"
author: "Agust√≠n Valencia"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(mboost)
library(randomForest)
```


# 1. Ensemble Methods
The file spambase.csv contains information about the frequency of various words, characters, etc. for a total of 4601 e-mails. Furthermore, these e-mails have been classified as spams (spam = 1) or regular e-mails (spam = 0). You can find more information about these data at https://archive.ics.uci.edu/ml/datasets/Spambase. 

Your task is to evaluate the performance of Adaboost classification trees and random forests on the spam data. Specifically, provide a plot showing the error rates when the number of trees considered are 10, 20, . . . , 100. To estimate the error rates, use 2/3 of the data for training and 1/3 as hold-out test data.

To learn Adaboost classification trees, use the function blackboost() of the R package mboost. Specify the loss function corresponding to Adaboost with the parameter family. To learn random forests, use the function randomForest of the R package randomForest. To load the data, you may want to use the following code:

```{r ensemble}
sp <- read.csv2("data/spambase.csv")
sp$Spam <- as.factor(sp$Spam)
```

## Solution

```{r}
## Splitting data
set.seed(42)
n <- dim(sp)[1]
idxs <- sample(1:n, floor(2*n/3))
train <- sp[idxs,]
test <- sp[-idxs,]

# Training
formula <- Spam ~ .
control <- ctree

bb <- blackboost(Spam ~ ., 
                 data = train, 
                 family = Binomial(type = "adaboost") )

predicted <- predict(bb, test, type = "class")
true <- test$Spam
confussion <- table(true, predicted)
tn <- confussion[1,1]
tp <- confussion[2,2]
fn <- confussion[1,2]
fp <- confussion[2,1]
total <- sum(confussion)
success <- (tp + tn) / total * 100
miss <- (fp + fn) / total * 100
```


